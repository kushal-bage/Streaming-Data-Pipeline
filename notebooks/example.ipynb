{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Real Time Data Engineering Projects**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ (end-to-end) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á data engineering pipeline ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ingestion, streaming, processing ‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô real-time processing ‡πÅ‡∏•‡∏∞ containerized deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏∞‡∏ö‡∏ö (System Architecture)**\n",
    "\n",
    "‡∏°‡∏µ component ‡∏´‡∏•‡∏±‡∏Å‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n",
    "\n",
    "- **Data Source**: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡πÄ‡∏ä‡πà‡∏ô randomuser.me ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á (dummy data) \n",
    "\n",
    "- **Apache Airflow**: ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà orchestration ‡∏Ç‡∏≠‡∏á pipeline ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà fetch ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô PostgreSQL \n",
    "\n",
    "- **Kafka + Zookeeper**: ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö streaming ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å PostgreSQL ‡πÑ‡∏õ‡∏¢‡∏±‡∏á processing engine \n",
    "\n",
    "- **Control Center & Schema Registry**: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏• schema ‡πÅ‡∏•‡∏∞ monitoring Kafka streams \n",
    "\n",
    "- **Apache Spark**: ‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (stream processing) ‡∏ú‡πà‡∏≤‡∏ô cluster (master/worker nodes) \n",
    "\n",
    "- **Cassandra**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### **‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Orchestration: Apache Airflow\n",
    "\n",
    "- Streaming: Apache Kafka & Zookeeper\n",
    "\n",
    "- Processing: Apache Spark\n",
    "\n",
    "- Storage: PostgreSQL (‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•), Cassandra (‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•)\n",
    "\n",
    "- Containerization: Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå **Kafka ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Data Engineer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Kafka ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**\n",
    "\n",
    "- Kafka = ‡∏£‡∏∞‡∏ö‡∏ö distributed event streaming platform\n",
    "  ‚Üí ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á, ‡πÄ‡∏Å‡πá‡∏ö ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö real-time\n",
    "\n",
    "- ‡∏°‡∏±‡∏ô‡∏ó‡∏≥‡∏ï‡∏±‡∏ß‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô message broker (‡∏ó‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏≤‡∏á)\n",
    "\n",
    "- ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞‡πÉ‡∏ô‡∏á‡∏≤‡∏ô real-time analytics, log streaming, data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å (Concepts)**\n",
    "\n",
    "‡∏•‡∏≠‡∏á‡∏Ñ‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô \"‡∏´‡πâ‡∏≠‡∏á chat\" üëá\n",
    "\n",
    "- **Producer** ‚Üí ‡∏Ñ‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Kafka)\n",
    "\n",
    "- **Consumer** ‚Üí ‡∏Ñ‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏ä‡πâ)\n",
    "\n",
    "- **Topic** ‚Üí ‡∏´‡πâ‡∏≠‡∏á‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ (channel ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏á/‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)\n",
    "\n",
    "- **Broker** ‚Üí Server ‡∏Ç‡∏≠‡∏á Kafka ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö topic\n",
    "\n",
    "- **Cluster** ‚Üí ‡∏Å‡∏•‡∏∏‡πà‡∏° broker ‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏ï‡∏±‡∏ß\n",
    "\n",
    "- **Partition** ‚Üí ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á topic ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠ scale (‡πÅ‡∏ï‡πà‡∏•‡∏∞ partition ‡∏Ñ‡∏∑‡∏≠‡∏ó‡πà‡∏≠‡πÅ‡∏¢‡∏Å)\n",
    "\n",
    "- **Offset** ‚Üí ‡πÄ‡∏•‡∏Ç‡∏ö‡∏≠‡∏Å‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á message ‡πÉ‡∏ô partition\n",
    "\n",
    "üëâ Keyword: Publish‚ÄìSubscribe system\n",
    "Producer ‡∏¢‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‚Üí Kafka ‚Üí Consumer ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. Kafka Data Flow**\n",
    "\n",
    "1. Producer ‡∏™‡∏£‡πâ‡∏≤‡∏á event ‚Üí ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏ó‡∏µ‡πà Kafka topic\n",
    "\n",
    "2. Kafka ‡πÄ‡∏Å‡πá‡∏ö event ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô log ‡πÅ‡∏ö‡∏ö append-only\n",
    "\n",
    "3. Consumer subscribe topic ‚Üí ‡∏≠‡πà‡∏≤‡∏ô event ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö offset\n",
    "\n",
    "4. Data ‡∏ñ‡∏π‡∏Å process ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ DB, Data Lake, Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Ñ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö version ‡∏Ç‡∏≠‡∏á Python ‡πÅ‡∏•‡∏∞ Spark ‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å Docker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.6\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.28\n",
      "Branch HEAD\n",
      "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
      "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!python3 --version\n",
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Mock Streaming data ‡∏à‡∏≤‡∏Å API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_data():\n",
    "    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Random User\n",
    "    res = requests.get(\"https://randomuser.me/api/\") # ‡πÄ‡∏ß‡∏•‡∏≤‡∏î‡∏∂‡∏á data ‡∏°‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô\n",
    "    res = res.json() # ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å json ‡πÄ‡∏õ‡πá‡∏ô dict\n",
    "    res = res['results'][0]\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"gender\": \"female\",\n",
      "   \"name\": {\n",
      "      \"title\": \"Mrs\",\n",
      "      \"first\": \"Joelma\",\n",
      "      \"last\": \"Carvalho\"\n",
      "   },\n",
      "   \"location\": {\n",
      "      \"street\": {\n",
      "         \"number\": 273,\n",
      "         \"name\": \"Rua Para\\u00edba \"\n",
      "      },\n",
      "      \"city\": \"Paulista\",\n",
      "      \"state\": \"Esp\\u00edrito Santo\",\n",
      "      \"country\": \"Brazil\",\n",
      "      \"postcode\": 84055,\n",
      "      \"coordinates\": {\n",
      "         \"latitude\": \"-75.3848\",\n",
      "         \"longitude\": \"135.3140\"\n",
      "      },\n",
      "      \"timezone\": {\n",
      "         \"offset\": \"-3:30\",\n",
      "         \"description\": \"Newfoundland\"\n",
      "      }\n",
      "   },\n",
      "   \"email\": \"joelma.carvalho@example.com\",\n",
      "   \"login\": {\n",
      "      \"uuid\": \"729407a4-d45a-4518-8981-922ee498a995\",\n",
      "      \"username\": \"bigmeercat213\",\n",
      "      \"password\": \"keywest\",\n",
      "      \"salt\": \"rtPueIVL\",\n",
      "      \"md5\": \"1228c6e9945b11b9af65cc28194a311e\",\n",
      "      \"sha1\": \"4c3a15a09c1ed12010c7ebffae10e9275aea64c4\",\n",
      "      \"sha256\": \"897de8f0e17b825aac6637456af02458d25639734e0341147b52b369e2af40dc\"\n",
      "   },\n",
      "   \"dob\": {\n",
      "      \"date\": \"1952-11-11T17:30:51.966Z\",\n",
      "      \"age\": 72\n",
      "   },\n",
      "   \"registered\": {\n",
      "      \"date\": \"2020-12-23T23:09:17.112Z\",\n",
      "      \"age\": 4\n",
      "   },\n",
      "   \"phone\": \"(25) 5666-3763\",\n",
      "   \"cell\": \"(74) 6689-0756\",\n",
      "   \"id\": {\n",
      "      \"name\": \"CPF\",\n",
      "      \"value\": \"473.270.720-90\"\n",
      "   },\n",
      "   \"picture\": {\n",
      "      \"large\": \"https://randomuser.me/api/portraits/women/85.jpg\",\n",
      "      \"medium\": \"https://randomuser.me/api/portraits/med/women/85.jpg\",\n",
      "      \"thumbnail\": \"https://randomuser.me/api/portraits/thumb/women/85.jpg\"\n",
      "   },\n",
      "   \"nat\": \"BR\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(get_data(), indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def format_data(res):\n",
    "    # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    data = {}\n",
    "    location = res['location']\n",
    "    data['id'] = str(uuid.uuid4())\n",
    "    data['first_name'] = res['name']['first']\n",
    "    data['last_name'] = res['name']['last']\n",
    "    data['gender'] = res['gender']\n",
    "    data['address'] = f\"{str(location['street']['number'])} {location['street']['name']}, \" \\\n",
    "                      f\"{location['city']}, {location['state']}, {location['country']}\"\n",
    "    data['post_code'] = location['postcode']\n",
    "    data['email'] = res['email']\n",
    "    data['username'] = res['login']['username']\n",
    "    data['dob'] = res['dob']['date']\n",
    "    data['registered_date'] = res['registered']['date']\n",
    "    data['phone'] = res['phone']\n",
    "    data['picture'] = res['picture']['medium']\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '3be8f393-e386-417c-8ebf-3cdccfaae80b', 'first_name': 'Hugo', 'last_name': 'Moreno', 'gender': 'male', 'address': '6139 Calle del Pez, Santa Cruz de Tenerife, Arag√≥n, Spain', 'post_code': 48348, 'email': 'hugo.moreno@example.com', 'username': 'lazydog173', 'dob': '1980-12-06T16:12:22.549Z', 'registered_date': '2012-01-16T04:28:50.357Z', 'phone': '954-268-334', 'picture': 'https://randomuser.me/api/portraits/med/men/12.jpg'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '3be8f393-e386-417c-8ebf-3cdccfaae80b',\n",
       " 'first_name': 'Hugo',\n",
       " 'last_name': 'Moreno',\n",
       " 'gender': 'male',\n",
       " 'address': '6139 Calle del Pez, Santa Cruz de Tenerife, Arag√≥n, Spain',\n",
       " 'post_code': 48348,\n",
       " 'email': 'hugo.moreno@example.com',\n",
       " 'username': 'lazydog173',\n",
       " 'dob': '1980-12-06T16:12:22.549Z',\n",
       " 'registered_date': '2012-01-16T04:28:50.357Z',\n",
       " 'phone': '954-268-334',\n",
       " 'picture': 'https://randomuser.me/api/portraits/med/men/12.jpg'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_data(get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mock Api Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_data():\n",
    "    import json\n",
    "    from kafka import KafkaProducer\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    producer = KafkaProducer(bootstrap_servers=['broker:29092'], max_block_ms=5000) # ‡∏™‡∏£‡πâ‡∏≤‡∏á KafkaProducer ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏ä‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡πÉ‡∏ô Kafka Boker\n",
    "    curr_time = time.time() # real time\n",
    "\n",
    "    while True:\n",
    "        if time.time() > curr_time + 10: # 1 minute\n",
    "            break\n",
    "        try:\n",
    "            res = get_data()\n",
    "            res = format_data(res)\n",
    "\n",
    "            print(res)\n",
    "            producer.send('users_created', json.dumps(res).encode('utf-8')) # ‡∏™‡πà‡∏á res ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô json ‡πÑ‡∏õ‡πÉ‡∏´‡πâ boker\n",
    "        except Exception as e:\n",
    "            logging.error(f'An error occured: {e}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'a1ec5ca8-7d47-4d47-bed4-96ff198a9a65', 'first_name': 'Katharine', 'last_name': 'Pott', 'gender': 'female', 'address': '3259 Tannenweg, Freyung, Baden-W√ºrttemberg, Germany', 'post_code': 35617, 'email': 'katharine.pott@example.com', 'username': 'heavymouse564', 'dob': '1990-04-01T04:00:24.010Z', 'registered_date': '2022-03-02T22:58:10.484Z', 'phone': '0875-8014851', 'picture': 'https://randomuser.me/api/portraits/med/women/63.jpg'}\n",
      "{'id': 'a1ec5ca8-7d47-4d47-bed4-96ff198a9a65', 'first_name': 'Katharine', 'last_name': 'Pott', 'gender': 'female', 'address': '3259 Tannenweg, Freyung, Baden-W√ºrttemberg, Germany', 'post_code': 35617, 'email': 'katharine.pott@example.com', 'username': 'heavymouse564', 'dob': '1990-04-01T04:00:24.010Z', 'registered_date': '2022-03-02T22:58:10.484Z', 'phone': '0875-8014851', 'picture': 'https://randomuser.me/api/portraits/med/women/63.jpg'}\n",
      "{'id': '9b4bd941-d9d1-4487-8386-193e6d2fd478', 'first_name': 'Vilma', 'last_name': 'Makela', 'gender': 'female', 'address': '5078 Reijolankatu, Pyh√§joki, Central Finland, Finland', 'post_code': 15921, 'email': 'vilma.makela@example.com', 'username': 'yellowpanda345', 'dob': '1961-11-07T00:55:42.145Z', 'registered_date': '2013-06-05T13:11:50.915Z', 'phone': '07-483-687', 'picture': 'https://randomuser.me/api/portraits/med/women/1.jpg'}\n",
      "{'id': '9b4bd941-d9d1-4487-8386-193e6d2fd478', 'first_name': 'Vilma', 'last_name': 'Makela', 'gender': 'female', 'address': '5078 Reijolankatu, Pyh√§joki, Central Finland, Finland', 'post_code': 15921, 'email': 'vilma.makela@example.com', 'username': 'yellowpanda345', 'dob': '1961-11-07T00:55:42.145Z', 'registered_date': '2013-06-05T13:11:50.915Z', 'phone': '07-483-687', 'picture': 'https://randomuser.me/api/portraits/med/women/1.jpg'}\n",
      "{'id': 'd196549a-deeb-432a-a6a5-75306c6b4878', 'first_name': 'Charles', 'last_name': 'Ryan', 'gender': 'male', 'address': '9732 School Lane, Monaghan, South Dublin, Ireland', 'post_code': 70970, 'email': 'charles.ryan@example.com', 'username': 'ticklishzebra476', 'dob': '1964-05-13T13:04:05.858Z', 'registered_date': '2004-06-04T07:43:29.933Z', 'phone': '011-631-8870', 'picture': 'https://randomuser.me/api/portraits/med/men/90.jpg'}\n",
      "{'id': 'd196549a-deeb-432a-a6a5-75306c6b4878', 'first_name': 'Charles', 'last_name': 'Ryan', 'gender': 'male', 'address': '9732 School Lane, Monaghan, South Dublin, Ireland', 'post_code': 70970, 'email': 'charles.ryan@example.com', 'username': 'ticklishzebra476', 'dob': '1964-05-13T13:04:05.858Z', 'registered_date': '2004-06-04T07:43:29.933Z', 'phone': '011-631-8870', 'picture': 'https://randomuser.me/api/portraits/med/men/90.jpg'}\n",
      "{'id': '66953662-0cd1-485a-8871-f919ede85510', 'first_name': 'Vincent', 'last_name': 'Smith', 'gender': 'male', 'address': '2058 Main Street, Palmerston North, Tasman, New Zealand', 'post_code': 90986, 'email': 'vincent.smith@example.com', 'username': 'yellowmeercat619', 'dob': '1958-01-26T00:21:05.310Z', 'registered_date': '2015-08-04T01:18:58.409Z', 'phone': '(698)-055-2859', 'picture': 'https://randomuser.me/api/portraits/med/men/42.jpg'}\n",
      "{'id': '66953662-0cd1-485a-8871-f919ede85510', 'first_name': 'Vincent', 'last_name': 'Smith', 'gender': 'male', 'address': '2058 Main Street, Palmerston North, Tasman, New Zealand', 'post_code': 90986, 'email': 'vincent.smith@example.com', 'username': 'yellowmeercat619', 'dob': '1958-01-26T00:21:05.310Z', 'registered_date': '2015-08-04T01:18:58.409Z', 'phone': '(698)-055-2859', 'picture': 'https://randomuser.me/api/portraits/med/men/42.jpg'}\n",
      "{'id': '91a7ca7c-16a7-43f3-82f7-b148f847381c', 'first_name': 'Austin', 'last_name': 'Warren', 'gender': 'male', 'address': '5567 Wycliff Ave, El Paso, Georgia, United States', 'post_code': 32460, 'email': 'austin.warren@example.com', 'username': 'greenmouse947', 'dob': '1950-07-13T19:59:58.211Z', 'registered_date': '2019-01-25T23:15:28.848Z', 'phone': '(830) 605-9643', 'picture': 'https://randomuser.me/api/portraits/med/men/51.jpg'}\n",
      "{'id': '91a7ca7c-16a7-43f3-82f7-b148f847381c', 'first_name': 'Austin', 'last_name': 'Warren', 'gender': 'male', 'address': '5567 Wycliff Ave, El Paso, Georgia, United States', 'post_code': 32460, 'email': 'austin.warren@example.com', 'username': 'greenmouse947', 'dob': '1950-07-13T19:59:58.211Z', 'registered_date': '2019-01-25T23:15:28.848Z', 'phone': '(830) 605-9643', 'picture': 'https://randomuser.me/api/portraits/med/men/51.jpg'}\n",
      "{'id': '9088a838-1794-4cc1-9569-d58e9462db96', 'first_name': 'Chad', 'last_name': 'Moore', 'gender': 'male', 'address': '1771 Plum St, Perth, Northern Territory, Australia', 'post_code': 6354, 'email': 'chad.moore@example.com', 'username': 'orangegorilla433', 'dob': '1997-11-18T14:17:19.618Z', 'registered_date': '2005-01-31T01:19:54.764Z', 'phone': '04-9606-0480', 'picture': 'https://randomuser.me/api/portraits/med/men/64.jpg'}\n",
      "{'id': '9088a838-1794-4cc1-9569-d58e9462db96', 'first_name': 'Chad', 'last_name': 'Moore', 'gender': 'male', 'address': '1771 Plum St, Perth, Northern Territory, Australia', 'post_code': 6354, 'email': 'chad.moore@example.com', 'username': 'orangegorilla433', 'dob': '1997-11-18T14:17:19.618Z', 'registered_date': '2005-01-31T01:19:54.764Z', 'phone': '04-9606-0480', 'picture': 'https://randomuser.me/api/portraits/med/men/64.jpg'}\n",
      "{'id': '0833c9be-5ec9-47c5-83f5-0dea941875fe', 'first_name': 'Isaac', 'last_name': 'Reyes', 'gender': 'male', 'address': '9757 Avenida de Burgos, Guadalajara, Comunidad Valenciana, Spain', 'post_code': 87874, 'email': 'isaac.reyes@example.com', 'username': 'beautifulsnake376', 'dob': '1975-07-22T22:45:33.668Z', 'registered_date': '2002-12-19T21:16:37.222Z', 'phone': '902-417-990', 'picture': 'https://randomuser.me/api/portraits/med/men/90.jpg'}\n",
      "{'id': '0833c9be-5ec9-47c5-83f5-0dea941875fe', 'first_name': 'Isaac', 'last_name': 'Reyes', 'gender': 'male', 'address': '9757 Avenida de Burgos, Guadalajara, Comunidad Valenciana, Spain', 'post_code': 87874, 'email': 'isaac.reyes@example.com', 'username': 'beautifulsnake376', 'dob': '1975-07-22T22:45:33.668Z', 'registered_date': '2002-12-19T21:16:37.222Z', 'phone': '902-417-990', 'picture': 'https://randomuser.me/api/portraits/med/men/90.jpg'}\n",
      "{'id': '3b6550ed-4c2e-497c-bc52-e4500a658460', 'first_name': 'Vandana', 'last_name': 'Baartmans', 'gender': 'female', 'address': '2299 Harfsenstraat, Agelo, Utrecht, Netherlands', 'post_code': '2460 KB', 'email': 'vandana.baartmans@example.com', 'username': 'whiteleopard579', 'dob': '1959-05-14T01:53:51.357Z', 'registered_date': '2014-08-23T20:01:43.170Z', 'phone': '(0447) 984187', 'picture': 'https://randomuser.me/api/portraits/med/women/46.jpg'}\n",
      "{'id': '3b6550ed-4c2e-497c-bc52-e4500a658460', 'first_name': 'Vandana', 'last_name': 'Baartmans', 'gender': 'female', 'address': '2299 Harfsenstraat, Agelo, Utrecht, Netherlands', 'post_code': '2460 KB', 'email': 'vandana.baartmans@example.com', 'username': 'whiteleopard579', 'dob': '1959-05-14T01:53:51.357Z', 'registered_date': '2014-08-23T20:01:43.170Z', 'phone': '(0447) 984187', 'picture': 'https://randomuser.me/api/portraits/med/women/46.jpg'}\n",
      "{'id': 'a9d0bc7a-12b1-4fa8-a334-77641a88fe17', 'first_name': 'Afet', 'last_name': 'Ayka√ß', 'gender': 'female', 'address': '8117 ≈ûehitler Cd, Kayseri, Osmaniye, Turkey', 'post_code': 24796, 'email': 'afet.aykac@example.com', 'username': 'bigcat191', 'dob': '1974-03-10T19:57:37.219Z', 'registered_date': '2005-04-10T16:47:03.285Z', 'phone': '(135)-511-3322', 'picture': 'https://randomuser.me/api/portraits/med/women/49.jpg'}\n",
      "{'id': 'a9d0bc7a-12b1-4fa8-a334-77641a88fe17', 'first_name': 'Afet', 'last_name': 'Ayka√ß', 'gender': 'female', 'address': '8117 ≈ûehitler Cd, Kayseri, Osmaniye, Turkey', 'post_code': 24796, 'email': 'afet.aykac@example.com', 'username': 'bigcat191', 'dob': '1974-03-10T19:57:37.219Z', 'registered_date': '2005-04-10T16:47:03.285Z', 'phone': '(135)-511-3322', 'picture': 'https://randomuser.me/api/portraits/med/women/49.jpg'}\n",
      "{'id': '78bd454b-a8e8-4f9d-b9d4-a39a4610ef28', 'first_name': 'Sinezora', 'last_name': 'Vinnik', 'gender': 'female', 'address': '9970 Lermontovska, Sosnivka, Rivnenska, Ukraine', 'post_code': 82051, 'email': 'sinezora.vinnik@example.com', 'username': 'yellowbear141', 'dob': '1965-01-07T13:52:00.834Z', 'registered_date': '2002-05-19T16:32:03.476Z', 'phone': '(068) K03-4536', 'picture': 'https://randomuser.me/api/portraits/med/women/15.jpg'}\n",
      "{'id': '78bd454b-a8e8-4f9d-b9d4-a39a4610ef28', 'first_name': 'Sinezora', 'last_name': 'Vinnik', 'gender': 'female', 'address': '9970 Lermontovska, Sosnivka, Rivnenska, Ukraine', 'post_code': 82051, 'email': 'sinezora.vinnik@example.com', 'username': 'yellowbear141', 'dob': '1965-01-07T13:52:00.834Z', 'registered_date': '2002-05-19T16:32:03.476Z', 'phone': '(068) K03-4536', 'picture': 'https://randomuser.me/api/portraits/med/women/15.jpg'}\n",
      "{'id': '86f6457a-76b8-4e0f-ba70-c09cfeeccf56', 'first_name': 'Elizabeth', 'last_name': 'Edwards', 'gender': 'female', 'address': '7504 Bairds Road, Timaru, Gisborne, New Zealand', 'post_code': 96993, 'email': 'elizabeth.edwards@example.com', 'username': 'redwolf799', 'dob': '1986-12-24T13:02:38.817Z', 'registered_date': '2016-06-08T01:33:21.047Z', 'phone': '(070)-164-4621', 'picture': 'https://randomuser.me/api/portraits/med/women/44.jpg'}\n",
      "{'id': '86f6457a-76b8-4e0f-ba70-c09cfeeccf56', 'first_name': 'Elizabeth', 'last_name': 'Edwards', 'gender': 'female', 'address': '7504 Bairds Road, Timaru, Gisborne, New Zealand', 'post_code': 96993, 'email': 'elizabeth.edwards@example.com', 'username': 'redwolf799', 'dob': '1986-12-24T13:02:38.817Z', 'registered_date': '2016-06-08T01:33:21.047Z', 'phone': '(070)-164-4621', 'picture': 'https://randomuser.me/api/portraits/med/women/44.jpg'}\n",
      "{'id': '89688c3b-8a6e-4705-b448-b33f135e3be2', 'first_name': 'Lina', 'last_name': 'Muller', 'gender': 'female', 'address': '3112 Rue Paul-Duvivier, Besan√ßon, Haute-Loire, France', 'post_code': 42797, 'email': 'lina.muller@example.com', 'username': 'silvermeercat795', 'dob': '1957-12-28T19:45:14.867Z', 'registered_date': '2008-07-07T03:24:22.985Z', 'phone': '04-50-68-99-18', 'picture': 'https://randomuser.me/api/portraits/med/women/33.jpg'}\n",
      "{'id': '89688c3b-8a6e-4705-b448-b33f135e3be2', 'first_name': 'Lina', 'last_name': 'Muller', 'gender': 'female', 'address': '3112 Rue Paul-Duvivier, Besan√ßon, Haute-Loire, France', 'post_code': 42797, 'email': 'lina.muller@example.com', 'username': 'silvermeercat795', 'dob': '1957-12-28T19:45:14.867Z', 'registered_date': '2008-07-07T03:24:22.985Z', 'phone': '04-50-68-99-18', 'picture': 'https://randomuser.me/api/portraits/med/women/33.jpg'}\n",
      "{'id': '317489f9-6b28-4003-ac13-ff005301a2af', 'first_name': 'Anja', 'last_name': 'Tadiƒá', 'gender': 'female', 'address': '304 Zorke Todosiƒá , Kuƒçevo, North Banat, Serbia', 'post_code': 92677, 'email': 'anja.tadic@example.com', 'username': 'happybird134', 'dob': '1945-11-01T01:26:40.652Z', 'registered_date': '2008-01-22T18:19:59.305Z', 'phone': '035-3122-509', 'picture': 'https://randomuser.me/api/portraits/med/women/13.jpg'}\n",
      "{'id': '317489f9-6b28-4003-ac13-ff005301a2af', 'first_name': 'Anja', 'last_name': 'Tadiƒá', 'gender': 'female', 'address': '304 Zorke Todosiƒá , Kuƒçevo, North Banat, Serbia', 'post_code': 92677, 'email': 'anja.tadic@example.com', 'username': 'happybird134', 'dob': '1945-11-01T01:26:40.652Z', 'registered_date': '2008-01-22T18:19:59.305Z', 'phone': '035-3122-509', 'picture': 'https://randomuser.me/api/portraits/med/women/13.jpg'}\n",
      "{'id': 'a538d518-0d42-4bc7-97a2-04efd8e4f729', 'first_name': 'Jennie', 'last_name': 'Malmin', 'gender': 'female', 'address': '3035 R√•dyrlia, Skarpengland, Aust-Agder, Norway', 'post_code': '2341', 'email': 'jennie.malmin@example.com', 'username': 'organicswan840', 'dob': '1972-11-07T06:33:16.229Z', 'registered_date': '2022-01-06T01:55:42.106Z', 'phone': '65547029', 'picture': 'https://randomuser.me/api/portraits/med/women/0.jpg'}\n",
      "{'id': 'a538d518-0d42-4bc7-97a2-04efd8e4f729', 'first_name': 'Jennie', 'last_name': 'Malmin', 'gender': 'female', 'address': '3035 R√•dyrlia, Skarpengland, Aust-Agder, Norway', 'post_code': '2341', 'email': 'jennie.malmin@example.com', 'username': 'organicswan840', 'dob': '1972-11-07T06:33:16.229Z', 'registered_date': '2022-01-06T01:55:42.106Z', 'phone': '65547029', 'picture': 'https://randomuser.me/api/portraits/med/women/0.jpg'}\n",
      "{'id': 'be2858cd-f7e5-4271-b275-557b1c8193f3', 'first_name': 'Corentin', 'last_name': 'Lefevre', 'gender': 'male', 'address': '8727 Rue des Ecrivains, Aulnay-sous-Bois, Bas-Rhin, France', 'post_code': 75667, 'email': 'corentin.lefevre@example.com', 'username': 'brownfrog865', 'dob': '1966-12-14T03:23:46.994Z', 'registered_date': '2009-08-13T01:49:36.727Z', 'phone': '05-19-61-16-65', 'picture': 'https://randomuser.me/api/portraits/med/men/36.jpg'}\n",
      "{'id': 'be2858cd-f7e5-4271-b275-557b1c8193f3', 'first_name': 'Corentin', 'last_name': 'Lefevre', 'gender': 'male', 'address': '8727 Rue des Ecrivains, Aulnay-sous-Bois, Bas-Rhin, France', 'post_code': 75667, 'email': 'corentin.lefevre@example.com', 'username': 'brownfrog865', 'dob': '1966-12-14T03:23:46.994Z', 'registered_date': '2009-08-13T01:49:36.727Z', 'phone': '05-19-61-16-65', 'picture': 'https://randomuser.me/api/portraits/med/men/36.jpg'}\n",
      "{'id': 'b7c07604-4de7-4b8c-adf2-22b13e5847ce', 'first_name': 'Ethan', 'last_name': 'Fields', 'gender': 'male', 'address': '1735 Westheimer Rd, Belen, Montana, United States', 'post_code': 49245, 'email': 'ethan.fields@example.com', 'username': 'blackladybug256', 'dob': '1961-12-11T03:03:45.402Z', 'registered_date': '2010-02-15T03:09:58.820Z', 'phone': '(534) 596-8474', 'picture': 'https://randomuser.me/api/portraits/med/men/85.jpg'}\n",
      "{'id': 'b7c07604-4de7-4b8c-adf2-22b13e5847ce', 'first_name': 'Ethan', 'last_name': 'Fields', 'gender': 'male', 'address': '1735 Westheimer Rd, Belen, Montana, United States', 'post_code': 49245, 'email': 'ethan.fields@example.com', 'username': 'blackladybug256', 'dob': '1961-12-11T03:03:45.402Z', 'registered_date': '2010-02-15T03:09:58.820Z', 'phone': '(534) 596-8474', 'picture': 'https://randomuser.me/api/portraits/med/men/85.jpg'}\n",
      "{'id': '4d061269-45d0-480f-80ca-fe132332f47b', 'first_name': 'Elisa', 'last_name': 'Gautier', 'gender': 'female', 'address': '1971 Rue de la Gare, N√Æmes, Marne, France', 'post_code': 78229, 'email': 'elisa.gautier@example.com', 'username': 'happydog135', 'dob': '2000-01-08T02:59:13.228Z', 'registered_date': '2020-04-30T02:15:10.690Z', 'phone': '01-58-23-07-88', 'picture': 'https://randomuser.me/api/portraits/med/women/69.jpg'}\n",
      "{'id': '4d061269-45d0-480f-80ca-fe132332f47b', 'first_name': 'Elisa', 'last_name': 'Gautier', 'gender': 'female', 'address': '1971 Rue de la Gare, N√Æmes, Marne, France', 'post_code': 78229, 'email': 'elisa.gautier@example.com', 'username': 'happydog135', 'dob': '2000-01-08T02:59:13.228Z', 'registered_date': '2020-04-30T02:15:10.690Z', 'phone': '01-58-23-07-88', 'picture': 'https://randomuser.me/api/portraits/med/women/69.jpg'}\n",
      "{'id': 'c041ea98-6888-41cc-9741-9919e08fcb1a', 'first_name': 'Gonzalo', 'last_name': 'Santill√°n', 'gender': 'male', 'address': '3918 Privada Seychelles, Palo Alto, Chiapas, Mexico', 'post_code': 80431, 'email': 'gonzalo.santillan@example.com', 'username': 'heavykoala100', 'dob': '2000-10-26T19:42:58.649Z', 'registered_date': '2019-09-20T17:30:45.373Z', 'phone': '(670) 236 8050', 'picture': 'https://randomuser.me/api/portraits/med/men/52.jpg'}\n",
      "{'id': 'c041ea98-6888-41cc-9741-9919e08fcb1a', 'first_name': 'Gonzalo', 'last_name': 'Santill√°n', 'gender': 'male', 'address': '3918 Privada Seychelles, Palo Alto, Chiapas, Mexico', 'post_code': 80431, 'email': 'gonzalo.santillan@example.com', 'username': 'heavykoala100', 'dob': '2000-10-26T19:42:58.649Z', 'registered_date': '2019-09-20T17:30:45.373Z', 'phone': '(670) 236 8050', 'picture': 'https://randomuser.me/api/portraits/med/men/52.jpg'}\n",
      "{'id': '89dce05e-91ce-4e13-bb98-de19ef70053a', 'first_name': 'Danilo', 'last_name': 'Roux', 'gender': 'male', 'address': '2633 Rue Andr√©-Gide, Soyhi√®res, Ticino, Switzerland', 'post_code': 1321, 'email': 'danilo.roux@example.com', 'username': 'whitemouse440', 'dob': '1979-08-23T22:31:42.016Z', 'registered_date': '2021-06-17T12:10:57.266Z', 'phone': '079 767 14 34', 'picture': 'https://randomuser.me/api/portraits/med/men/26.jpg'}\n",
      "{'id': '89dce05e-91ce-4e13-bb98-de19ef70053a', 'first_name': 'Danilo', 'last_name': 'Roux', 'gender': 'male', 'address': '2633 Rue Andr√©-Gide, Soyhi√®res, Ticino, Switzerland', 'post_code': 1321, 'email': 'danilo.roux@example.com', 'username': 'whitemouse440', 'dob': '1979-08-23T22:31:42.016Z', 'registered_date': '2021-06-17T12:10:57.266Z', 'phone': '079 767 14 34', 'picture': 'https://randomuser.me/api/portraits/med/men/26.jpg'}\n",
      "{'id': '9c4235d1-cc60-452c-8fbe-c3152054d83b', 'first_name': 'Bekim', 'last_name': 'Duval', 'gender': 'male', 'address': '2698 Rue Paul Bert, Marsens, Ticino, Switzerland', 'post_code': 2016, 'email': 'bekim.duval@example.com', 'username': 'bigsnake252', 'dob': '1991-08-18T14:30:05.196Z', 'registered_date': '2014-03-17T12:51:07.670Z', 'phone': '079 838 30 99', 'picture': 'https://randomuser.me/api/portraits/med/men/97.jpg'}\n",
      "{'id': '9c4235d1-cc60-452c-8fbe-c3152054d83b', 'first_name': 'Bekim', 'last_name': 'Duval', 'gender': 'male', 'address': '2698 Rue Paul Bert, Marsens, Ticino, Switzerland', 'post_code': 2016, 'email': 'bekim.duval@example.com', 'username': 'bigsnake252', 'dob': '1991-08-18T14:30:05.196Z', 'registered_date': '2014-03-17T12:51:07.670Z', 'phone': '079 838 30 99', 'picture': 'https://randomuser.me/api/portraits/med/men/97.jpg'}\n",
      "{'id': 'e0e45632-95ba-43cf-b804-2bf2bfb0351c', 'first_name': 'Angelique', 'last_name': 'Schmied', 'gender': 'female', 'address': '6222 Kastanienweg, Cottbus, Saarland, Germany', 'post_code': 78956, 'email': 'angelique.schmied@example.com', 'username': 'beautifulgorilla165', 'dob': '1965-07-02T06:28:20.232Z', 'registered_date': '2019-12-07T07:16:37.309Z', 'phone': '0783-8278075', 'picture': 'https://randomuser.me/api/portraits/med/women/61.jpg'}\n",
      "{'id': 'e0e45632-95ba-43cf-b804-2bf2bfb0351c', 'first_name': 'Angelique', 'last_name': 'Schmied', 'gender': 'female', 'address': '6222 Kastanienweg, Cottbus, Saarland, Germany', 'post_code': 78956, 'email': 'angelique.schmied@example.com', 'username': 'beautifulgorilla165', 'dob': '1965-07-02T06:28:20.232Z', 'registered_date': '2019-12-07T07:16:37.309Z', 'phone': '0783-8278075', 'picture': 'https://randomuser.me/api/portraits/med/women/61.jpg'}\n",
      "{'id': '7f482850-74ac-443e-bb6a-e8ba9f2a4932', 'first_name': 'Marta', 'last_name': 'Delgado', 'gender': 'female', 'address': '7057 Calle de Arturo Soria, Barcelona, Galicia, Spain', 'post_code': 45748, 'email': 'marta.delgado@example.com', 'username': 'angrypanda359', 'dob': '1968-01-25T20:34:02.492Z', 'registered_date': '2006-11-04T04:51:27.735Z', 'phone': '939-869-091', 'picture': 'https://randomuser.me/api/portraits/med/women/18.jpg'}\n",
      "{'id': '7f482850-74ac-443e-bb6a-e8ba9f2a4932', 'first_name': 'Marta', 'last_name': 'Delgado', 'gender': 'female', 'address': '7057 Calle de Arturo Soria, Barcelona, Galicia, Spain', 'post_code': 45748, 'email': 'marta.delgado@example.com', 'username': 'angrypanda359', 'dob': '1968-01-25T20:34:02.492Z', 'registered_date': '2006-11-04T04:51:27.735Z', 'phone': '939-869-091', 'picture': 'https://randomuser.me/api/portraits/med/women/18.jpg'}\n"
     ]
    }
   ],
   "source": [
    "# Function Streaming Data from API\n",
    "stream_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ETL (Extract, Transform, Load) ‡πÅ‡∏ö‡∏ö Real-time** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏Ñ‡∏∑‡∏≠ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (user) ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô Kafka ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á, ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á, ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Spark\n",
    "def create_spark_connection():\n",
    "    s_conn = None\n",
    "\n",
    "    try:\n",
    "        s_conn = (SparkSession.builder \n",
    "            .appName('SparkDataStreaming') \n",
    "            .config('spark.jars.packages', # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏û‡∏Ñ‡πÄ‡∏Å‡∏à‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Spark\n",
    "                    \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0,\" # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏û‡∏Ñ‡πÄ‡∏Å‡∏à‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cassandra\n",
    "                    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏û‡∏Ñ‡πÄ‡∏Å‡∏à‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Kafka \n",
    "            .config('spark.cassandra.connection.host', 'localhost') # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cassandra\n",
    "            .getOrCreate())\n",
    "        \n",
    "        s_conn.sparkContext.setLogLevel(\"ERROR\")\n",
    "        logging.info(\"Spark connection created successfully!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Couldn't create the spark session due to exception {e}\")\n",
    "\n",
    "    return s_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Kafka\n",
    "def connect_to_kafka(spark_conn):\n",
    "    spark_df = None\n",
    "    try:\n",
    "        spark_df = (spark_conn.readStream \\\n",
    "            .format('kafka') \\\n",
    "            .option('kafka.bootstrap.servers', 'broker:29092') # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Kafka Broker\n",
    "            .option('subscribe', 'users_created') # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á Kafka Topic ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤\n",
    "            .option('startingOffsets', 'earliest') # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà earliest (‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô) ‡∏´‡∏£‡∏∑‡∏≠ latest (‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)\n",
    "            .option(\"maxOffsetsPerTrigger\", 100)  # ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡∏•‡∏∞ 50 records\n",
    "            .load())\n",
    "        logging.info(\"kafka dataframe created successfully\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"kafka dataframe could not be created because: {e}\")\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cassandra\n",
    "def create_cassandra_connection():\n",
    "    try:\n",
    "        # connecting to the cassandra cluster\n",
    "        cluster = Cluster(contact_points=['cassandra'], port=9042) # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà hostname ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cassandra\n",
    "        cas_session = cluster.connect() # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠\n",
    "\n",
    "        logging.info(\"Connected to Cassandra!\")\n",
    "        return cas_session\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not create cassandra connection due to {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keyspace(session):\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Database / Schema ‡∏ö‡∏ô Cassandra ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Keyspace\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS spark_streams\n",
    "        WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Keyspace created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(session):\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Table\n",
    "    session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS spark_streams.created_users (\n",
    "        id UUID PRIMARY KEY,\n",
    "        first_name TEXT,\n",
    "        last_name TEXT,\n",
    "        gender TEXT,\n",
    "        address TEXT,\n",
    "        post_code TEXT,\n",
    "        email TEXT,\n",
    "        username TEXT,\n",
    "        registered_date TEXT,\n",
    "        phone TEXT,\n",
    "        picture TEXT) WITH default_time_to_live = 1200;\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Table created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ Spark ‡πÅ‡∏•‡∏∞ Data Quality Checks\n",
    "def create_selection_df_from_kafka(spark_df):\n",
    "    \"\"\"\n",
    "    ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Kafka ‡πÄ‡∏õ‡πá‡∏ô Spark DataFrame ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ Data Quality Checks\n",
    "    - Schema Enforcement\n",
    "    - Missing Value Check\n",
    "    - Business Rule Validation (regex, allowed values)\n",
    "    - Deduplication\n",
    "    - ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô validation ‡πÑ‡∏õ Dead-letter topic (optional)\n",
    "    \n",
    "    Args:\n",
    "        spark_df: input streaming DataFrame ‡∏à‡∏≤‡∏Å Kafka\n",
    "        kafka_producer_invalid: KafkaProducer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• invalid (optional)\n",
    "    \n",
    "    Returns:\n",
    "        validated_df: Spark DataFrame ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ (validated)\n",
    "    \"\"\"\n",
    "    # -------------------- 1. Define Schema --------------------\n",
    "    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"first_name\", StringType(), False),\n",
    "        StructField(\"last_name\", StringType(), False),\n",
    "        StructField(\"gender\", StringType(), False),\n",
    "        StructField(\"address\", StringType(), False),\n",
    "        StructField(\"post_code\", StringType(), False),\n",
    "        StructField(\"email\", StringType(), False),\n",
    "        StructField(\"username\", StringType(), False),\n",
    "        StructField(\"registered_date\", StringType(), False),\n",
    "        StructField(\"phone\", StringType(), False),\n",
    "        StructField(\"picture\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    # -------------------- 2. Parse JSON and Enforce Schema --------------------\n",
    "    # ‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å kafka ‡πÄ‡∏õ‡πá‡∏ô Spark DataFrame \n",
    "    df = (spark_df.selectExpr(\"CAST(value AS STRING)\") # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå value ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô String \n",
    "        .select(from_json(col('value'), schema).alias('data')).select(\"data.*\")) # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å json ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô value ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° schema\n",
    "    logging.info(\"Schema applied and JSON parsed.\")\n",
    "\n",
    "    # -------------------- 3. Missing Value Check --------------------\n",
    "    df_non_null = df.dropna(subset=[\"id\", \"email\", \"registered_date\"]) # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô null ‡πÉ‡∏ô col ‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ\n",
    "    logging.info(\"Dropped records with null id, email, or registered_date.\")\n",
    "\n",
    "    # -------------------- 4. Business Rule Validation --------------------\n",
    "    df_valid = df_non_null.filter( # filter ‡πÅ‡∏Ñ‡πà‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏° regex ‡∏ï‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ\n",
    "        col(\"email\").rlike(r\".+@.+\\..+\") & # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á email pattern\n",
    "        col(\"gender\").isin(\"male\", \"female\") & # ‡∏à‡∏∞‡∏°‡∏µ‡πÅ‡∏Ñ‡πà 2 ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "        col(\"id\").rlike(\"^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$\")\n",
    "    )\n",
    "    logging.info(\"Applied business rule validation.\")\n",
    "\n",
    "    # -------------------- 5. Deduplicate --------------------\n",
    "    df_deduped = df_valid.dropDuplicates([\"id\"]) # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ id ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô\n",
    "    logging.info(\"Deduplicated based on id.\")\n",
    "\n",
    "    return df_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5bf06d28-90e7-4ab5-9e78-d5080ba9b157;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.5.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.5.0/spark-cassandra-connector_2.12-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.5.0!spark-cassandra-connector_2.12.jar (10446ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar (4456ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.5.0/spark-cassandra-connector-driver_2.12-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0!spark-cassandra-connector-driver_2.12.jar (1516ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.11.0/scala-collection-compat_2.12-2.11.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.11.0!scala-collection-compat_2.12.jar (3267ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (5978ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (644ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (9641ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (5987ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (3884ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (1437ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (2299ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (1948ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (3492ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (2413ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (1173ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (621ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (1803ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (1771ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (936ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (2921ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (4092ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (467ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (6369ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (490ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (715ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (426ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (2540ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (445ms)\n",
      ":: resolution report :: resolve 119673ms :: artifacts dl 82216ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.5.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   30  |   29  |   29  |   2   ||   28  |   28  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5bf06d28-90e7-4ab5-9e78-d5080ba9b157\n",
      "\tconfs: [default]\n",
      "\t28 artifacts copied, 0 already retrieved (75061kB/134ms)\n",
      "25/09/11 04:12:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-09-11 04:12:09,771 - INFO - Spark connection created successfully!\n",
      "2025-09-11 04:12:13,022 - INFO - kafka dataframe created successfully\n",
      "2025-09-11 04:12:13,738 - INFO - Schema applied and JSON parsed.\n",
      "2025-09-11 04:12:13,777 - INFO - Dropped records with null id, email, or registered_date.\n",
      "2025-09-11 04:12:13,850 - INFO - Applied business rule validation.\n",
      "2025-09-11 04:12:13,872 - INFO - Deduplicated based on id.\n",
      "2025-09-11 04:12:13,884 - WARNING - Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['cassandra'], lbp = None)\n",
      "2025-09-11 04:12:14,440 - WARNING - Downgrading core protocol version from 66 to 65 for 172.18.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "2025-09-11 04:12:14,444 - WARNING - Downgrading core protocol version from 65 to 5 for 172.18.0.4:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "2025-09-11 04:12:14,466 - INFO - Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '172.18.0.4:9042'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\n",
      "2025-09-11 04:12:14,527 - INFO - Connected to Cassandra!\n",
      "2025-09-11 04:12:14,559 - INFO - Streaming is being started...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyspace created successfully!\n",
      "Table created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/11 04:12:50 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@e5220bc,com.datastax.spark.connector.cql.CassandraConnector@7e2a89fa,TableDef(spark_streams,created_users,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,UUIDType)),ArrayBuffer(),Stream(ColumnDef(address,RegularColumn,VarCharType), ColumnDef(email,RegularColumn,VarCharType), ColumnDef(first_name,RegularColumn,VarCharType), ColumnDef(gender,RegularColumn,VarCharType), ColumnDef(last_name,RegularColumn,VarCharType), ColumnDef(phone,RegularColumn,VarCharType), ColumnDef(picture,RegularColumn,VarCharType), ColumnDef(post_code,RegularColumn,VarCharType), ColumnDef(registered_date,RegularColumn,VarCharType), ColumnDef(username,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(gender,StringType,true),StructField(address,StringType,true),StructField(post_code,StringType,true),StructField(email,StringType,true),StructField(username,StringType,true),StructField(registered_date,StringType,true),StructField(phone,StringType,true),StructField(picture,StringType,true)),org.apache.spark.SparkConf@163aa5eb)] is aborting.\n",
      "25/09/11 04:12:50 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: CassandraBulkWrite(org.apache.spark.sql.SparkSession@e5220bc,com.datastax.spark.connector.cql.CassandraConnector@7e2a89fa,TableDef(spark_streams,created_users,ArrayBuffer(ColumnDef(id,PartitionKeyColumn,UUIDType)),ArrayBuffer(),Stream(ColumnDef(address,RegularColumn,VarCharType), ColumnDef(email,RegularColumn,VarCharType), ColumnDef(first_name,RegularColumn,VarCharType), ColumnDef(gender,RegularColumn,VarCharType), ColumnDef(last_name,RegularColumn,VarCharType), ColumnDef(phone,RegularColumn,VarCharType), ColumnDef(picture,RegularColumn,VarCharType), ColumnDef(post_code,RegularColumn,VarCharType), ColumnDef(registered_date,RegularColumn,VarCharType), ColumnDef(username,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(id,StringType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(gender,StringType,true),StructField(address,StringType,true),StructField(post_code,StringType,true),StructField(email,StringType,true),StructField(username,StringType,true),StructField(registered_date,StringType,true),StructField(phone,StringType,true),StructField(picture,StringType,true)),org.apache.spark.SparkConf@163aa5eb)] aborted.\n",
      "25/09/11 04:12:51 ERROR Utils: Aborting task                    (107 + 8) / 200]\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/09/11 04:12:51 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Commit denied for partition 108 (task 511, attempt 0, stage 5.0).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:915)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/09/11 04:12:51 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Commit denied for partition 107 (task 510, attempt 0, stage 5.0).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:915)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborting commit for partition 107 (task 510, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborting commit for partition 114 (task 517, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborting commit for partition 108 (task 511, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborted commit for partition 107 (task 510, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborted commit for partition 108 (task 511, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborted commit for partition 114 (task 517, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Commit denied for partition 110 (task 513, attempt 0, stage 5.0).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:915)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/09/11 04:12:51 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Commit denied for partition 113 (task 516, attempt 0, stage 5.0).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:915)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborting commit for partition 113 (task 516, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborting commit for partition 110 (task 513, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborted commit for partition 113 (task 516, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborted commit for partition 110 (task 513, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Commit denied for partition 111 (task 514, attempt 0, stage 5.0).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:915)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborting commit for partition 111 (task 514, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Commit denied for partition 112 (task 515, attempt 0, stage 5.0).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:915)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborting commit for partition 112 (task 515, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborted commit for partition 112 (task 515, attempt 0, stage 5.0)\n",
      "25/09/11 04:12:51 ERROR DataWritingSparkTask: Aborted commit for partition 111 (task 514, attempt 0, stage 5.0)\n"
     ]
    }
   ],
   "source": [
    "# create spark connection\n",
    "spark_conn = create_spark_connection() # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Spark\n",
    "\n",
    "if spark_conn is not None:\n",
    "        # connect to kafka with spark connection\n",
    "        spark_df = connect_to_kafka(spark_conn) # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Kafka\n",
    "        selection_df = create_selection_df_from_kafka(spark_df) # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Kafka\n",
    "        session = create_cassandra_connection() # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cassandra\n",
    "\n",
    "        # Testing Data Flow on Console\n",
    "        # query = (selection_df\n",
    "        #  .writeStream\n",
    "        #  .format(\"console\")\n",
    "        #  .outputMode(\"append\")\n",
    "        #  .start())\n",
    "\n",
    "        # query.awaitTermination(30) # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ 30s ‡πÉ‡∏´‡πâ query data\n",
    "        # query.stop() # stop streaming query ‡∏´‡∏•‡∏±‡∏á 30s\n",
    "\n",
    "        if session is not None:\n",
    "            create_keyspace(session) # ‡∏™‡∏£‡πâ‡∏≤‡∏á Database / Schema ‡∏ö‡∏ô Cassandra ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Keyspace\n",
    "            create_table(session) # ‡∏™‡∏£‡πâ‡∏≤‡∏á Table\n",
    "\n",
    "            logging.info(\"Streaming is being started...\")\n",
    "\n",
    "            # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Database\n",
    "            streaming_query = (selection_df.writeStream\n",
    "                               .format(\"org.apache.spark.sql.cassandra\")\n",
    "                               .option('checkpointLocation', '/tmp/checkpoint')\n",
    "                               .option('keyspace', 'spark_streams')\n",
    "                               .option('table', 'created_users')\n",
    "                               .option(\"spark.cassandra.connection.host\", \"cassandra\")\n",
    "                               .option(\"spark.cassandra.connection.port\", \"9042\")\n",
    "                               .option(\"spark.cassandra.connection.local_dc\", \"datacenter1\")\n",
    "                               .trigger(processingTime=\"10 seconds\") # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏∏‡∏Å‡πÜ 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n",
    "                               .start())\n",
    "\n",
    "            streaming_query.awaitTermination(30) # ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡πÅ‡∏•‡πâ‡∏ß query data\n",
    "            streaming_query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
