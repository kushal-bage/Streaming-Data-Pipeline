{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Real Time Data Engineering Projects**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ (end-to-end) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á data engineering pipeline ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ingestion, streaming, processing ‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô real-time processing ‡πÅ‡∏•‡∏∞ containerized deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏∞‡∏ö‡∏ö (System Architecture)**\n",
    "\n",
    "‡∏°‡∏µ component ‡∏´‡∏•‡∏±‡∏Å‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n",
    "\n",
    "- **Data Source**: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡πÄ‡∏ä‡πà‡∏ô randomuser.me ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á (dummy data) \n",
    "\n",
    "- **Apache Airflow**: ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà orchestration ‡∏Ç‡∏≠‡∏á pipeline ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà fetch ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô PostgreSQL \n",
    "\n",
    "- **Kafka + Zookeeper**: ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö streaming ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å PostgreSQL ‡πÑ‡∏õ‡∏¢‡∏±‡∏á processing engine \n",
    "\n",
    "- **Control Center & Schema Registry**: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏• schema ‡πÅ‡∏•‡∏∞ monitoring Kafka streams \n",
    "\n",
    "- **Apache Spark**: ‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (stream processing) ‡∏ú‡πà‡∏≤‡∏ô cluster (master/worker nodes) \n",
    "\n",
    "- **Cassandra**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### **‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Orchestration: Apache Airflow\n",
    "\n",
    "- Streaming: Apache Kafka & Zookeeper\n",
    "\n",
    "- Processing: Apache Spark\n",
    "\n",
    "- Storage: PostgreSQL (‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•), Cassandra (‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•)\n",
    "\n",
    "- Containerization: Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå **Kafka ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Data Engineer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Kafka ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**\n",
    "\n",
    "- Kafka = ‡∏£‡∏∞‡∏ö‡∏ö distributed event streaming platform\n",
    "  ‚Üí ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á, ‡πÄ‡∏Å‡πá‡∏ö ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö real-time\n",
    "\n",
    "- ‡∏°‡∏±‡∏ô‡∏ó‡∏≥‡∏ï‡∏±‡∏ß‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô message broker (‡∏ó‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏≤‡∏á)\n",
    "\n",
    "- ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞‡πÉ‡∏ô‡∏á‡∏≤‡∏ô real-time analytics, log streaming, data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å (Concepts)**\n",
    "\n",
    "‡∏•‡∏≠‡∏á‡∏Ñ‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô \"‡∏´‡πâ‡∏≠‡∏á chat\" üëá\n",
    "\n",
    "- **Producer** ‚Üí ‡∏Ñ‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Kafka)\n",
    "\n",
    "- **Consumer** ‚Üí ‡∏Ñ‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏ä‡πâ)\n",
    "\n",
    "- **Topic** ‚Üí ‡∏´‡πâ‡∏≠‡∏á‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ (channel ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏á/‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)\n",
    "\n",
    "- **Broker** ‚Üí Server ‡∏Ç‡∏≠‡∏á Kafka ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö topic\n",
    "\n",
    "- **Cluster** ‚Üí ‡∏Å‡∏•‡∏∏‡πà‡∏° broker ‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏ï‡∏±‡∏ß\n",
    "\n",
    "- **Partition** ‚Üí ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á topic ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠ scale (‡πÅ‡∏ï‡πà‡∏•‡∏∞ partition ‡∏Ñ‡∏∑‡∏≠‡∏ó‡πà‡∏≠‡πÅ‡∏¢‡∏Å)\n",
    "\n",
    "- **Offset** ‚Üí ‡πÄ‡∏•‡∏Ç‡∏ö‡∏≠‡∏Å‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á message ‡πÉ‡∏ô partition\n",
    "\n",
    "üëâ Keyword: Publish‚ÄìSubscribe system\n",
    "Producer ‡∏¢‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‚Üí Kafka ‚Üí Consumer ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. Kafka Data Flow**\n",
    "\n",
    "1. Producer ‡∏™‡∏£‡πâ‡∏≤‡∏á event ‚Üí ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏ó‡∏µ‡πà Kafka topic\n",
    "\n",
    "2. Kafka ‡πÄ‡∏Å‡πá‡∏ö event ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô log ‡πÅ‡∏ö‡∏ö append-only\n",
    "\n",
    "3. Consumer subscribe topic ‚Üí ‡∏≠‡πà‡∏≤‡∏ô event ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö offset\n",
    "\n",
    "4. Data ‡∏ñ‡∏π‡∏Å process ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ DB, Data Lake, Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Ñ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö version ‡∏Ç‡∏≠‡∏á Python ‡πÅ‡∏•‡∏∞ Spark ‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å Docker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 --version\n",
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Mock Streaming data ‡∏à‡∏≤‡∏Å API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_data():\n",
    "    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Random User\n",
    "    res = requests.get(\"https://randomuser.me/api/\") # ‡πÄ‡∏ß‡∏•‡∏≤‡∏î‡∏∂‡∏á data ‡∏°‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô\n",
    "    res = res.json() # ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å json ‡πÄ‡∏õ‡πá‡∏ô dict\n",
    "    res = res['results'][0]\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(get_data(), indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def format_data(res):\n",
    "    # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "    data = {}\n",
    "    location = res['location']\n",
    "    data['id'] = str(uuid.uuid4())\n",
    "    data['first_name'] = res['name']['first']\n",
    "    data['last_name'] = res['name']['last']\n",
    "    data['gender'] = res['gender']\n",
    "    data['address'] = f\"{str(location['street']['number'])} {location['street']['name']}, \" \\\n",
    "                      f\"{location['city']}, {location['state']}, {location['country']}\"\n",
    "    data['post_code'] = location['postcode']\n",
    "    data['email'] = res['email']\n",
    "    data['username'] = res['login']['username']\n",
    "    data['dob'] = res['dob']['date']\n",
    "    data['registered_date'] = res['registered']['date']\n",
    "    data['phone'] = res['phone']\n",
    "    data['picture'] = res['picture']['medium']\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_data(get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mock Api Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_data():\n",
    "    import json\n",
    "    from kafka import KafkaProducer\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    producer = KafkaProducer(bootstrap_servers=['broker:29092'], max_block_ms=5000) # ‡∏™‡∏£‡πâ‡∏≤‡∏á KafkaProducer ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏ä‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡πÉ‡∏ô Kafka Boker\n",
    "    curr_time = time.time() # real time\n",
    "\n",
    "    while True:\n",
    "        if time.time() > curr_time + 10: # 1 minute\n",
    "            break\n",
    "        try:\n",
    "            res = get_data()\n",
    "            res = format_data(res)\n",
    "\n",
    "            print(res)\n",
    "            producer.send('users_created', json.dumps(res).encode('utf-8')) # ‡∏™‡πà‡∏á res ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô json ‡πÑ‡∏õ‡πÉ‡∏´‡πâ boker\n",
    "        except Exception as e:\n",
    "            logging.error(f'An error occured: {e}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Streaming Data from API\n",
    "stream_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ETL (Extract, Transform, Load) ‡πÅ‡∏ö‡∏ö Real-time** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏Ñ‡∏∑‡∏≠ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (user) ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô Kafka ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á, ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á, ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Spark\n",
    "def create_spark_connection():\n",
    "    s_conn = None\n",
    "\n",
    "    try:\n",
    "        s_conn = (SparkSession.builder \n",
    "            .appName('SparkDataStreaming') \n",
    "            .config('spark.jars.packages', # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏û‡∏Ñ‡πÄ‡∏Å‡∏à‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Spark\n",
    "                    \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0,\" # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏û‡∏Ñ‡πÄ‡∏Å‡∏à‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cassandra\n",
    "                    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏û‡∏Ñ‡πÄ‡∏Å‡∏à‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Kafka \n",
    "            .config('spark.cassandra.connection.host', 'localhost') # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cassandra\n",
    "            .getOrCreate())\n",
    "        \n",
    "        s_conn.sparkContext.setLogLevel(\"ERROR\")\n",
    "        logging.info(\"Spark connection created successfully!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Couldn't create the spark session due to exception {e}\")\n",
    "\n",
    "    return s_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Kafka\n",
    "def connect_to_kafka(spark_conn):\n",
    "    spark_df = None\n",
    "    try:\n",
    "        spark_df = (spark_conn.readStream \\\n",
    "            .format('kafka') \\\n",
    "            .option('kafka.bootstrap.servers', 'broker:29092') # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Kafka Broker\n",
    "            .option('subscribe', 'users_created') # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á Kafka Topic ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤\n",
    "            .option('startingOffsets', 'earliest') # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà earliest (‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô) ‡∏´‡∏£‡∏∑‡∏≠ latest (‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)\n",
    "            .option(\"maxOffsetsPerTrigger\", 100)  # ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡∏•‡∏∞ 50 records\n",
    "            .load())\n",
    "        logging.info(\"kafka dataframe created successfully\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"kafka dataframe could not be created because: {e}\")\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cassandra\n",
    "def create_cassandra_connection():\n",
    "    try:\n",
    "        # connecting to the cassandra cluster\n",
    "        cluster = Cluster(contact_points=['cassandra'], port=9042) # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà hostname ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cassandra\n",
    "        cas_session = cluster.connect() # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠\n",
    "\n",
    "        logging.info(\"Connected to Cassandra!\")\n",
    "        return cas_session\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not create cassandra connection due to {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keyspace(session):\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Database / Schema ‡∏ö‡∏ô Cassandra ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Keyspace\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS spark_streams\n",
    "        WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Keyspace created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(session):\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Table\n",
    "    session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS spark_streams.created_users (\n",
    "        id UUID PRIMARY KEY,\n",
    "        first_name TEXT,\n",
    "        last_name TEXT,\n",
    "        gender TEXT,\n",
    "        address TEXT,\n",
    "        post_code TEXT,\n",
    "        email TEXT,\n",
    "        username TEXT,\n",
    "        registered_date TEXT,\n",
    "        phone TEXT,\n",
    "        picture TEXT) WITH default_time_to_live = 1200;\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Table created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ Spark ‡πÅ‡∏•‡∏∞ Data Quality Checks\n",
    "def create_selection_df_from_kafka(spark_df):\n",
    "    \"\"\"\n",
    "    ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Kafka ‡πÄ‡∏õ‡πá‡∏ô Spark DataFrame ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ Data Quality Checks\n",
    "    - Schema Enforcement\n",
    "    - Missing Value Check\n",
    "    - Business Rule Validation (regex, allowed values)\n",
    "    - Deduplication\n",
    "    - ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô validation ‡πÑ‡∏õ Dead-letter topic (optional)\n",
    "    \n",
    "    Args:\n",
    "        spark_df: input streaming DataFrame ‡∏à‡∏≤‡∏Å Kafka\n",
    "        kafka_producer_invalid: KafkaProducer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• invalid (optional)\n",
    "    \n",
    "    Returns:\n",
    "        validated_df: Spark DataFrame ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ (validated)\n",
    "    \"\"\"\n",
    "    # -------------------- 1. Define Schema --------------------\n",
    "    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"first_name\", StringType(), False),\n",
    "        StructField(\"last_name\", StringType(), False),\n",
    "        StructField(\"gender\", StringType(), False),\n",
    "        StructField(\"address\", StringType(), False),\n",
    "        StructField(\"post_code\", StringType(), False),\n",
    "        StructField(\"email\", StringType(), False),\n",
    "        StructField(\"username\", StringType(), False),\n",
    "        StructField(\"registered_date\", StringType(), False),\n",
    "        StructField(\"phone\", StringType(), False),\n",
    "        StructField(\"picture\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    # -------------------- 2. Parse JSON and Enforce Schema --------------------\n",
    "    # ‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å kafka ‡πÄ‡∏õ‡πá‡∏ô Spark DataFrame \n",
    "    df = (spark_df.selectExpr(\"CAST(value AS STRING)\") # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå value ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô String \n",
    "        .select(from_json(col('value'), schema).alias('data')).select(\"data.*\")) # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å json ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô value ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° schema\n",
    "    logging.info(\"Schema applied and JSON parsed.\")\n",
    "\n",
    "    # -------------------- 3. Missing Value Check --------------------\n",
    "    df_non_null = df.dropna(subset=[\"id\", \"email\", \"registered_date\"]) # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô null ‡πÉ‡∏ô col ‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ\n",
    "    logging.info(\"Dropped records with null id, email, or registered_date.\")\n",
    "\n",
    "    # -------------------- 4. Business Rule Validation --------------------\n",
    "    df_valid = df_non_null.filter( # filter ‡πÅ‡∏Ñ‡πà‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏° regex ‡∏ï‡∏≤‡∏°‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ\n",
    "        col(\"email\").rlike(r\".+@.+\\..+\") & # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á email pattern\n",
    "        col(\"gender\").isin(\"male\", \"female\") & # ‡∏à‡∏∞‡∏°‡∏µ‡πÅ‡∏Ñ‡πà 2 ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "        col(\"id\").rlike(\"^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$\")\n",
    "    )\n",
    "    logging.info(\"Applied business rule validation.\")\n",
    "\n",
    "    # -------------------- 5. Deduplicate --------------------\n",
    "    df_deduped = df_valid.dropDuplicates([\"id\"]) # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ id ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô\n",
    "    logging.info(\"Deduplicated based on id.\")\n",
    "\n",
    "    return df_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark connection\n",
    "spark_conn = create_spark_connection() # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Spark\n",
    "\n",
    "if spark_conn is not None:\n",
    "        # connect to kafka with spark connection\n",
    "        spark_df = connect_to_kafka(spark_conn) # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Kafka\n",
    "        selection_df = create_selection_df_from_kafka(spark_df) # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Kafka\n",
    "        session = create_cassandra_connection() # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Cassandra\n",
    "\n",
    "        # Testing Data Flow on Console\n",
    "        # query = (selection_df\n",
    "        #  .writeStream\n",
    "        #  .format(\"console\")\n",
    "        #  .outputMode(\"append\")\n",
    "        #  .start())\n",
    "\n",
    "        # query.awaitTermination(30) # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ 30s ‡πÉ‡∏´‡πâ query data\n",
    "        # query.stop() # stop streaming query ‡∏´‡∏•‡∏±‡∏á 30s\n",
    "\n",
    "        if session is not None:\n",
    "            create_keyspace(session) # ‡∏™‡∏£‡πâ‡∏≤‡∏á Database / Schema ‡∏ö‡∏ô Cassandra ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Keyspace\n",
    "            create_table(session) # ‡∏™‡∏£‡πâ‡∏≤‡∏á Table\n",
    "\n",
    "            logging.info(\"Streaming is being started...\")\n",
    "\n",
    "            # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Database\n",
    "            streaming_query = (selection_df.writeStream\n",
    "                               .format(\"org.apache.spark.sql.cassandra\")\n",
    "                               .option('checkpointLocation', '/tmp/checkpoint')\n",
    "                               .option('keyspace', 'spark_streams')\n",
    "                               .option('table', 'created_users')\n",
    "                               .option(\"spark.cassandra.connection.host\", \"cassandra\")\n",
    "                               .option(\"spark.cassandra.connection.port\", \"9042\")\n",
    "                               .option(\"spark.cassandra.connection.local_dc\", \"datacenter1\")\n",
    "                               .trigger(processingTime=\"10 seconds\") # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏∏‡∏Å‡πÜ 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n",
    "                               .start())\n",
    "\n",
    "            streaming_query.awaitTermination(30) # ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡πÅ‡∏•‡πâ‡∏ß query data\n",
    "            streaming_query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
